{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfff020-287a-47f4-9420-a808cb283740",
   "metadata": {},
   "source": [
    "# AI Application in HEP Tutorial (Supervised): Classification\n",
    "\n",
    "In the previous tutorial, we went through a simple regression task where we attempted to use ML algorithms to model the invarint mass of the heavy particle using the jet kinematic info. Another major application of ML in HEP is classification, as quite often the task is to distinguish signal from background.\n",
    "\n",
    "In this tutorial, we will see how to do solve a classification problem in HEP. We will try to identify heavy particle decays with multijet events as the background. Since we have introduced the necessary SW tricks and details in the last tutorial, we will skip them this time. \n",
    "\n",
    "The example in this tutorial is only considering one mass point (1 TeV Z'). More signal points are provided for you to exercise further as homework. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7077f89e-317b-47ea-8eee-3867ccb5cb91",
   "metadata": {},
   "source": [
    "## Step I: Import libraries and load the data\n",
    "\n",
    "Everything is the same as the last tutorial except we have to also load the background data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a9e69e-29a6-42f8-bc09-a50ac7831f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import *\n",
    "from tqdm import tqdm\n",
    "import uproot as r\n",
    "import awkward as ak\n",
    "import vector\n",
    "\n",
    "# Use the following syntax to read a delphes root file and parse the branches to arrays\n",
    "# You might want to open the test file using ROOT to see the structure. \n",
    "# What is the tree name? How are all the branches defined?\n",
    "\n",
    "signal = r.open(\"./data/delphes_zprime_1TeV.root\")\n",
    "delphes_tree_signal = signal[\"Delphes;1\"]\n",
    "background = r.open(\"./data/delphes_multijet.root\")\n",
    "delphes_tree_background = background[\"Delphes;1\"]\n",
    "\n",
    "pt_signal = delphes_tree_signal[\"Jet.PT\"].array()\n",
    "eta_signal = delphes_tree_signal[\"Jet.Eta\"].array()\n",
    "phi_signal = delphes_tree_signal[\"Jet.Phi\"].array()\n",
    "m_signal = delphes_tree_signal[\"Jet.Mass\"].array()\n",
    "\n",
    "pt_background = delphes_tree_background[\"Jet.PT\"].array()\n",
    "eta_background = delphes_tree_background[\"Jet.Eta\"].array()\n",
    "phi_background = delphes_tree_background[\"Jet.Phi\"].array()\n",
    "m_background = delphes_tree_background[\"Jet.Mass\"].array()\n",
    "\n",
    "# Here we use the vector (https://github.com/scikit-hep/vector) library\n",
    "# It allows us to use its implemented Lorentz vector functions.\n",
    "\n",
    "vector.register_awkward()\n",
    "\n",
    "jet_vec_signal = ak.zip({\n",
    "  \"pt\": pt_signal,\n",
    "  \"phi\": phi_signal,\n",
    "  \"eta\": eta_signal,\n",
    "  \"mass\": m_signal,\n",
    "},with_name=\"Momentum4D\")\n",
    "\n",
    "jet_vec_background = ak.zip({\n",
    "  \"pt\": pt_background,\n",
    "  \"phi\": phi_background,\n",
    "  \"eta\": eta_background,\n",
    "  \"mass\": m_background,\n",
    "},with_name=\"Momentum4D\")   \n",
    "\n",
    "jet_vec_select_signal = jet_vec_signal[ak.num(jet_vec_signal) >= 2] \n",
    "jet_vec_select_background = jet_vec_background[ak.num(jet_vec_background) >= 2] \n",
    "\n",
    "lead_jet_signal = jet_vec_select_signal[:,0] # https://stackoverflow.com/questions/16815928/what-does-mean-on-numpy-arrays\n",
    "sublead_jet_signal = jet_vec_select_signal[:,1]\n",
    "\n",
    "dijet_signal = lead_jet_signal + sublead_jet_signal\n",
    "dijet_mass_signal = dijet_signal.mass\n",
    "\n",
    "lead_jet_background = jet_vec_select_background[:,0] \n",
    "sublead_jet_background = jet_vec_select_background[:,1]\n",
    "\n",
    "dijet_background = lead_jet_background + sublead_jet_background\n",
    "dijet_mass_background = dijet_background.mass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da04d09-912b-4d06-a139-6b70820ca1a2",
   "metadata": {},
   "source": [
    "## Step II: Visualizing the data\n",
    "\n",
    "As usually, let's check the data. Please notice that we do have both the signal and background processes. We should compare the properties.\n",
    "\n",
    "**Question:** Which quantities have the best discriminating power?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ad57c-db7a-407b-a385-b9a0ea577344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.hist(pt_signal[ak.num(pt_signal) >= 1][:,0],bins=20,range=(0,1000),density=True,\n",
    "         label='Signal',fill=False,edgecolor='red',linewidth=3,histtype='step');\n",
    "plt.hist(pt_background[ak.num(pt_background) >= 1][:,0],bins=20,range=(0,1000),density=True,\n",
    "         label='Background',fill=False,edgecolor='blue',linewidth=3,histtype='step');\n",
    "plt.xlabel(r'Leading Jet $p_T$ (GeV/c)',fontsize=14)\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.show()\n",
    "\n",
    "plt.hist(dijet_mass_signal,bins=20,range=(0,2000),density=True,\n",
    "         label='Signal',fill=False,edgecolor='red',linewidth=3,histtype='step');\n",
    "plt.hist(dijet_mass_background,bins=20,range=(0,2000),density=True,\n",
    "         label='Background',fill=False,edgecolor='blue',linewidth=3,histtype='step');\n",
    "plt.xlabel(r'Mass  (GeV/c)',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c22be-cedb-418f-ad04-5871c2a9bae8",
   "metadata": {},
   "source": [
    "**Homework**: \n",
    "\n",
    "**1**: Compare other quantities between signal and background, and identify possible candidates to select (veto) signal (background).\n",
    "\n",
    "**2**: What variables can be constructed for the di-jet system besides the invariant mass?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60843d98-6498-4155-9c8a-12cf6cf52d54",
   "metadata": {},
   "source": [
    "## Step III: Save datasets for ML\n",
    "\n",
    "We can either save various separate files and then combine them when loading the datasets, or combine the datasets before saving them.\n",
    "\n",
    "**Question**: Can you try the latter approach, combining the datasets first and then saving them as a signal csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278ffa1-93e1-4b5b-8f2e-243fe2076f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_jet_pt_signal = ak.to_numpy(lead_jet_signal.pt).reshape(1,-1)\n",
    "lead_jet_eta_signal = ak.to_numpy(lead_jet_signal.eta).reshape(1,-1)\n",
    "lead_jet_phi_signal = ak.to_numpy(lead_jet_signal.phi).reshape(1,-1)\n",
    "lead_jet_e_signal = ak.to_numpy(lead_jet_signal.e).reshape(1,-1)\n",
    "sublead_jet_pt_signal = ak.to_numpy(sublead_jet_signal.pt).reshape(1,-1)\n",
    "sublead_jet_eta_signal = ak.to_numpy(sublead_jet_signal.eta).reshape(1,-1)\n",
    "sublead_jet_phi_signal = ak.to_numpy(sublead_jet_signal.phi).reshape(1,-1)\n",
    "sublead_jet_e_signal = ak.to_numpy(sublead_jet_signal.e).reshape(1,-1)\n",
    "dijet_mass_signal = ak.to_numpy(dijet_mass_signal).reshape(1,-1)\n",
    "label_signal = np.ones(sublead_jet_e_signal.shape, dtype = int)\n",
    "\n",
    "dataset_signal = np.row_stack((lead_jet_pt_signal, lead_jet_eta_signal, lead_jet_phi_signal, \n",
    "                          lead_jet_e_signal, sublead_jet_pt_signal, sublead_jet_eta_signal, \n",
    "                          sublead_jet_phi_signal, sublead_jet_e_signal, dijet_mass_signal, label_signal))\n",
    "\n",
    "dataset_df_signal = pd.DataFrame(np.transpose(dataset_signal), columns=['jet1_pt', 'jet1_eta', 'jet1_phi', 'jet1_e', \n",
    "                                                          'jet2_pt', 'jet2_eta', 'jet2_phi', 'jet2_e', 'mass', 'label']) \n",
    "\n",
    "dataset_df_signal.to_csv(\"dataset_signal.csv\")\n",
    "\n",
    "# Now let's deal with the background\n",
    "\n",
    "lead_jet_pt_background = ak.to_numpy(lead_jet_background.pt).reshape(1,-1)\n",
    "lead_jet_eta_background = ak.to_numpy(lead_jet_background.eta).reshape(1,-1)\n",
    "lead_jet_phi_background = ak.to_numpy(lead_jet_background.phi).reshape(1,-1)\n",
    "lead_jet_e_background = ak.to_numpy(lead_jet_background.e).reshape(1,-1)\n",
    "sublead_jet_pt_background = ak.to_numpy(sublead_jet_background.pt).reshape(1,-1)\n",
    "sublead_jet_eta_background = ak.to_numpy(sublead_jet_background.eta).reshape(1,-1)\n",
    "sublead_jet_phi_background = ak.to_numpy(sublead_jet_background.phi).reshape(1,-1)\n",
    "sublead_jet_e_background = ak.to_numpy(sublead_jet_background.e).reshape(1,-1)\n",
    "dijet_mass_background = ak.to_numpy(dijet_mass_background).reshape(1,-1)\n",
    "\n",
    "# Attention!!!\n",
    "label_background = np.zeros(sublead_jet_e_background.shape, dtype = int)\n",
    "\n",
    "dataset_background = np.row_stack((lead_jet_pt_background, lead_jet_eta_background, lead_jet_phi_background, \n",
    "                          lead_jet_e_background, sublead_jet_pt_background, sublead_jet_eta_background, \n",
    "                          sublead_jet_phi_background, sublead_jet_e_background, dijet_mass_background, label_background))\n",
    "\n",
    "dataset_df_background = pd.DataFrame(np.transpose(dataset_background), columns=['jet1_pt', 'jet1_eta', 'jet1_phi', 'jet1_e', \n",
    "                                                          'jet2_pt', 'jet2_eta', 'jet2_phi', 'jet2_e', 'mass', 'label']) \n",
    "\n",
    "dataset_df_background.to_csv(\"dataset_background.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07984cb-51a1-4090-a859-bcd8ba7d600b",
   "metadata": {},
   "source": [
    "## Step IV: Train the models\n",
    "\n",
    "We will again try three different approaches, linear model, BDT and neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1bb912-8c2d-403c-ae26-bab4b8faf8a5",
   "metadata": {},
   "source": [
    "### Linear model (logistic regression)\n",
    "\n",
    "Compared to a linear regression, a logistic regression differs only at the very last step. You do not need to change much when setting up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c2eee-6e17-437d-b77d-0756a5a4ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "\n",
    "df_signal = pd.read_csv('dataset_signal.csv')\n",
    "df_background = pd.read_csv('dataset_background.csv')\n",
    "\n",
    "df_combine = pd.concat([df_signal, df_background])\n",
    "df_combine.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "\n",
    "y = df_combine['label'].copy()\n",
    "X = df_combine.drop(['label'], axis=1).to_numpy()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,\n",
    "                                                    random_state=42) # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Call the logistic regression model. It is a class with various useful functions.\n",
    "log_regr = linear_model.LogisticRegression()\n",
    "\n",
    "log_regr.fit(X_train, y_train)\n",
    "\n",
    "#Test the model using the test dataset.\n",
    "y_pred = log_regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc826bc-c6c3-480e-954a-d517da876e2d",
   "metadata": {},
   "source": [
    "### Performance metric\n",
    "\n",
    "For the classification, we care about the four quantities: TN,TP,FP,FN\n",
    "\n",
    "**Question:** What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c1452-c088-4190-9f19-0a3c30ef19b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268c6c4-c9af-42f1-be47-9c69d7e894bf",
   "metadata": {},
   "source": [
    "### Boosted Regression Tree\n",
    "\n",
    "The same as above, we just need to swap the function to call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890e582-536a-4385-aa20-b5faa459b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas can falculate the covariance matrix for us and the seaborn library can make \n",
    "# very nice heatmaps (https://seaborn.pydata.org/).\n",
    "\n",
    "X_and_y = df_combine\n",
    "X_and_y.corr()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(X_and_y.corr(), cmap = \"YlGnBu\", annot=True)\n",
    "X_and_y.corr()['label'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032eedb4-a846-4d3d-8689-8a5140b5fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_cla = xgb.XGBClassifier(random_state=123)\n",
    "\n",
    "xgb_cla.get_params()\n",
    "xgb_cla.set_params(n_estimators=10)\n",
    "\n",
    "xgb_cla.fit(X_train, y_train)\n",
    "\n",
    "y_pred_bdt = xgb_cla.predict(X_test)\n",
    "\n",
    "xgb_cla.save_model(\"resonance_classifier.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ffec5-ff0a-434b-b842-864c16c24098",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_bdt)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c957e019-0580-446d-80f9-34cc16a346fe",
   "metadata": {},
   "source": [
    "#### Tree visualization\n",
    "\n",
    "As mentioned before, one great advantage of the boost tree is its interpretability. Now let's see how to use those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6b8d22-698a-4883-b523-5109d088355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8)\n",
    "\n",
    "xgb.plot_importance(xgb_cla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6255dd35-24ca-497e-b8a3-a481b89cfddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gain instead of weight\n",
    "xgb.plot_importance(xgb_cla, importance_type=\"gain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133d433-b4d7-4690-9a7d-45912e533a81",
   "metadata": {},
   "source": [
    "**Question**: are they as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6a682-9bda-49c5-8fe2-d1cd839a0745",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (20.0, 8)\n",
    "\n",
    "# Plot the first tree\n",
    "xgb.plot_tree(xgb_cla, num_trees=0)\n",
    "\n",
    "format = 'svg' \n",
    "\n",
    "image = xgb.to_graphviz(xgb_cla)\n",
    "\n",
    "#Set a different dpi (work only if format == 'png')\n",
    "image.graph_attr = {'dpi':'400'}\n",
    "\n",
    "image.render('bdt_classifier', format = format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ef6bc-b548-446f-9b5d-edcb7e1dbfa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb.plot_tree(xgb_cla, num_trees=9, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae5b89b-872a-4609-bf21-f3d7da6cfdfa",
   "metadata": {},
   "source": [
    "### Deep Neutral Network\n",
    "\n",
    "It is fairly easy as well to build a classifier using NN. We have to modify a few more steps here though. \n",
    "\n",
    "**Question**: What needs to be modified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb934cf-5a69-4e7e-b97a-19c947a56cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.optim as optim\n",
    "\n",
    "df_signal = pd.read_csv('dataset_signal.csv')\n",
    "df_background = pd.read_csv('dataset_background.csv')\n",
    "\n",
    "df_combine = pd.concat([df_signal, df_background])\n",
    "df_combine.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "\n",
    "y = df_combine['label'].copy()\n",
    "X = df_combine.drop(['label'], axis=1).to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle = True)\n",
    "X_train = X_train[:, [i for i in range(0,8)]]\n",
    "\n",
    "# Save a test dataset wich keeps the mass variable to get the mass easily afterwards\n",
    "X_test_keep_mass = X_test\n",
    "X_train_raw = X_train\n",
    "X_test_raw = X_test[:, [i for i in range(0,8)]]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71338ae9-232d-4027-824f-5dff323587bf",
   "metadata": {},
   "source": [
    "#### Loss function for classification\n",
    "\n",
    "The loss function applied in regresions is easy to understand as it represents the \"distance\" between the predicted and true values. For the classification, the loss function relies on imformation theory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb0cb4-850d-4fe5-bd1a-cf8a821dbffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetworkClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # The number of input nodes matches the number of total features\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            # The fully connected layers have to have compatible number of nodes.\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "        \n",
    "# loss function and optimizer\n",
    "model = NeuralNetworkClassification().to(device)\n",
    "loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87894d69-96b0-45b3-8ee9-ef0532d48d20",
   "metadata": {},
   "source": [
    "**Question**: What has been changed compared to the regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f14fba-6417-4d89-b92c-88781762d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "X_test_raw = torch.tensor(X_test_raw, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c811c9d-42ab-4f89-82b0-5a47a02eb6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import copy\n",
    "\n",
    "# training parameters\n",
    "n_epochs = 100   # number of epochs to run\n",
    "batch_size = 10  # size of each batch\n",
    "batch_start = torch.arange(0, len(X_train), batch_size) #find the location of each sample for each batch\n",
    " \n",
    "# Hold the best model\n",
    "best_acc = 0   # init to infinity, so the minimization will start decreasing the mse\n",
    "best_weights = None\n",
    "history = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch \" + str(epoch))\n",
    "    #Set the model to training mode. This is needed as some times the layers will behave differently in training \n",
    "    #and testing. Though in our exmaples both the RELU and Linear functions behace the same, it is a good practice to set the \n",
    "    #Model to training mode before it starts training. \n",
    "    model.train()\n",
    "    #Here we are using tqdm (https://tqdm.github.io/) to monitor the process but the effects can only bee seen in a terminal. \n",
    "    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for start in bar:\n",
    "            # take a batch\n",
    "            X_batch = X_train[start:start+batch_size].to(device)\n",
    "            y_batch = y_train[start:start+batch_size].to(device)\n",
    "            # forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            # calculate the loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # backward pass\n",
    "            # here we set zero_grad() why?\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update weights using one single step.\n",
    "            optimizer.step()\n",
    "            # Why does the following line work?\n",
    "            acc = (y_pred.round() == y_batch).float().mean()\n",
    "            bar.set_postfix(\n",
    "              loss=float(loss),\n",
    "              acc=float(acc)\n",
    "            )\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    acc = (y_pred.round() == y_test).float().mean()\n",
    "    acc = float(acc)\n",
    "    history.append(acc)\n",
    "    print(\"Accuracy: \" + str(acc))\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # restore model and return best accuracy\n",
    "model.load_state_dict(best_weights)\n",
    "# restore model and return best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3387b3e2-3534-4c9d-b49d-ab06d529d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf31e6-589a-47f0-bb55-4867ca7ee7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_nn.round().detach().numpy())\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "print(cm)\n",
    "print(\"Background fake rate: \" + str(cm[0,1]/(cm[0,1] + cm[0,0])))\n",
    "print(\"Signal accuracy: \" + str(cm[1,1]/(cm[1,1] + cm[1,0])))\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f365fe-5b21-4a96-a905-594a47f533b2",
   "metadata": {},
   "source": [
    "#### ROC curves\n",
    "\n",
    "When it comes to classification, you will see the so-called ROC curves very often. \n",
    "\n",
    "**Question:** Why do we need ROC curves? What is changing along the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bbe809-6f9f-4db5-81a0-9cb9be226166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_nn.detach().numpy())\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='NN Classification')\n",
    "\n",
    "display.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f4eeb-c73d-4f25-a31e-62b6749d7687",
   "metadata": {},
   "source": [
    "The classifier looks awesome....but\n",
    "\n",
    "Does it introduce biases?\n",
    "\n",
    "**Question:** What background events have been identified as signal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b5b6a-e039-42c1-9d5b-6417541fb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mask = y_pred_nn.round().detach().numpy().reshape(-1)\n",
    "true_mask = y_test.detach().numpy().reshape(-1)\n",
    "\n",
    "x_test_signal_mass = X_test_keep_mass[:,8][(true_mask == 1) & (pred_mask == 1)]\n",
    "x_test_background_mass = X_test_keep_mass[:,8][(true_mask == 0) & (pred_mask == 1)]\n",
    "\n",
    "plt.hist(x_test_signal_mass,bins=20,range=(0,2000),density=True,\n",
    "         label='Signal',fill=False,edgecolor='red',linewidth=3,histtype='step');\n",
    "plt.hist(x_test_background_mass,bins=20,range=(0,2000),density=True,\n",
    "         label='Background',fill=False,edgecolor='blue',linewidth=3,histtype='step');\n",
    "\n",
    "plt.xlabel(r'Mass (GeV/c)',fontsize=14)\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7217dc59-872a-4394-9657-b29293576904",
   "metadata": {},
   "source": [
    "#### Attention!!!\n",
    "\n",
    "Always be aware of the biases!\n",
    "\n",
    "ML can learn whatever it wants, and it is possible that it learns in a way we do not appreciate.\n",
    "\n",
    "**Question**: What can those biases introduce in actual data analyses?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde157c-5b44-49b7-ac52-f1bc2386b2d8",
   "metadata": {},
   "source": [
    "#### Ablation\n",
    "\n",
    "To mitigate the biases, there are many approaches to try. Let's see what happens if we remove the jet pT and energy variables in the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a0fd2-5914-44f0-a30e-e54d36094857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.optim as optim\n",
    "\n",
    "df_signal = pd.read_csv('dataset_signal.csv')\n",
    "df_background = pd.read_csv('dataset_background.csv')\n",
    "\n",
    "df_combine = pd.concat([df_signal, df_background])\n",
    "df_combine.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "\n",
    "y = df_combine['label'].copy()\n",
    "X = df_combine.drop(['label','jet1_pt', 'jet2_pt', 'jet1_e', 'jet2_e'], axis=1).to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle = True)\n",
    "X_train = X_train[:, [i for i in range(0,4)]]\n",
    "\n",
    "X_test_keep_mass = X_test\n",
    "X_train_raw = X_train\n",
    "X_test_raw = X_test[:, [i for i in range(0,4)]]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef8edd-2167-40b4-9501-fa5864a0ff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetworkClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # The number of input nodes matches the number of total features\n",
    "            nn.Linear(4, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "        \n",
    "# loss function and optimizer\n",
    "model = NeuralNetworkClassification().to(device)\n",
    "loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a35099-4961-408f-b194-e4b8dc4784b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "X_test_raw = torch.tensor(X_test_raw, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef64c4-cb71-41f1-a38e-d33c2e5fbd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import copy\n",
    "\n",
    "# training parameters\n",
    "n_epochs = 100   # number of epochs to run\n",
    "batch_size = 10  # size of each batch\n",
    "batch_start = torch.arange(0, len(X_train), batch_size) #find the location of each sample for each batch\n",
    " \n",
    "# Hold the best model\n",
    "best_acc = 0   # init to infinity, so the minimization will start decreasing the mse\n",
    "best_weights = None\n",
    "history = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch \" + str(epoch))\n",
    "    #Set the model to training mode. This is needed as some times the layers will behave differently in training \n",
    "    #and testing. Though in our exmaples both the RELU and Linear functions behace the same, it is a good practice to set the \n",
    "    #Model to training mode before it starts training. \n",
    "    model.train()\n",
    "    #Here we are using tqdm (https://tqdm.github.io/) to monitor the process but the effects can only bee seen in a terminal. \n",
    "    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for start in bar:\n",
    "            # take a batch\n",
    "            X_batch = X_train[start:start+batch_size].to(device)\n",
    "            y_batch = y_train[start:start+batch_size].to(device)\n",
    "            # forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            # calculate the loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # backward pass\n",
    "            # here we set zero_grad() why?\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update weights using one single step.\n",
    "            optimizer.step()\n",
    "            # Why does the following line work?\n",
    "            acc = (y_pred.round() == y_batch).float().mean()\n",
    "            bar.set_postfix(\n",
    "              loss=float(loss),\n",
    "              acc=float(acc)\n",
    "            )\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    acc = (y_pred.round() == y_test).float().mean()\n",
    "    acc = float(acc)\n",
    "    history.append(acc)\n",
    "    print(\"Accuracy: \" + str(acc))\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # restore model and return best accuracy\n",
    "model.load_state_dict(best_weights)\n",
    "# restore model and return best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c916b7-e552-43de-b52a-15bc5ac6b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn = model(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_nn.round().detach().numpy())\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "print(cm)\n",
    "print(\"Background fake rate: \" + str(cm[0,1]/(cm[0,1] + cm[0,0])))\n",
    "print(\"Signal accuracy: \" + str(cm[1,1]/(cm[1,1] + cm[1,0])))\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db07d410-09ed-4f48-95bf-3e342c927411",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mask = y_pred_nn.round().detach().numpy().reshape(-1)\n",
    "true_mask = y_test.detach().numpy().reshape(-1)\n",
    "\n",
    "x_test_signal_mass = X_test_keep_mass[:,4][(true_mask == 1) & (pred_mask == 1)]\n",
    "x_test_background_mass = X_test_keep_mass[:,4][(true_mask == 0) & (pred_mask == 1)]\n",
    "\n",
    "plt.hist(x_test_signal_mass,bins=20,range=(0,2000),density=True,\n",
    "         label='Signal',fill=False,edgecolor='red',linewidth=3,histtype='step');\n",
    "plt.hist(x_test_background_mass,bins=20,range=(0,2000),density=True,\n",
    "         label='Background',fill=False,edgecolor='blue',linewidth=3,histtype='step');\n",
    "\n",
    "plt.xlabel(r'Mass (GeV/c)',fontsize=14)\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c20d9-c94f-41d1-b8dd-ac2e7f461dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_nn.detach().numpy())\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='NN Classification')\n",
    "\n",
    "display.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cfbe6c-1738-4405-9006-e3de3c2bd601",
   "metadata": {},
   "source": [
    "#### Impacts from training data.\n",
    "\n",
    "Let's think about it, we might only care about whether the algorithm can distinguish the signal from the background in a given mass region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa7d38-2846-414e-9bb5-6b734d13aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.optim as optim\n",
    "\n",
    "df_signal = pd.read_csv('dataset_signal.csv')\n",
    "df_background = pd.read_csv('dataset_background.csv')\n",
    "\n",
    "df_combine = pd.concat([df_signal, df_background])\n",
    "df_combine.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "\n",
    "y = df_combine['label'].copy()\n",
    "X = df_combine.drop(['label','jet1_pt', 'jet2_pt', 'jet1_e', 'jet2_e'], axis=1).to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle = True)\n",
    "\n",
    "y_train = y_train[(X_train[:,4] > 800) & (X_train[:,4] < 1100)]\n",
    "X_train = X_train[(X_train[:,4] > 800) & (X_train[:,4] < 1100)]\n",
    "X_train_raw = X_train[:, [i for i in range(0,4)]]\n",
    "\n",
    "X_test_keep_mass = X_test.copy()\n",
    "X_test_all_mass = X_test[:, [i for i in range(0,4)]]\n",
    "y_test_all_mass = y_test.copy()\n",
    "\n",
    "y_test = y_test[(X_test[:,4] > 800) & (X_test[:,4] < 1100)]\n",
    "X_test = X_test[(X_test[:,4] > 800) & (X_test[:,4] < 1100)]\n",
    "X_test_raw = X_test[:, [i for i in range(0,4)]]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetworkClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # The number of input nodes matches the number of total features\n",
    "            nn.Linear(4, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "        \n",
    "# loss function and optimizer\n",
    "model = NeuralNetworkClassification().to(device)\n",
    "loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "X_test_raw = torch.tensor(X_test_raw, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "X_test_all_mass = torch.tensor(X_test_all_mass, dtype=torch.float32).to(device)\n",
    "\n",
    "import tqdm\n",
    "import copy\n",
    "\n",
    "# training parameters\n",
    "n_epochs = 100   # number of epochs to run\n",
    "batch_size = 10  # size of each batch\n",
    "batch_start = torch.arange(0, len(X_train), batch_size) #find the location of each sample for each batch\n",
    " \n",
    "# Hold the best model\n",
    "best_acc = 0   # init to infinity, so the minimization will start decreasing the mse\n",
    "best_weights = None\n",
    "history = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch \" + str(epoch))\n",
    "    #Set the model to training mode. This is needed as some times the layers will behave differently in training \n",
    "    #and testing. Though in our exmaples both the RELU and Linear functions behace the same, it is a good practice to set the \n",
    "    #Model to training mode before it starts training. \n",
    "    model.train()\n",
    "    #Here we are using tqdm (https://tqdm.github.io/) to monitor the process but the effects can only bee seen in a terminal. \n",
    "    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for start in bar:\n",
    "            # take a batch\n",
    "            X_batch = X_train[start:start+batch_size].to(device)\n",
    "            y_batch = y_train[start:start+batch_size].to(device)\n",
    "            # forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            # calculate the loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # backward pass\n",
    "            # here we set zero_grad() why?\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update weights using one single step.\n",
    "            optimizer.step()\n",
    "            # Why does the following line work?\n",
    "            acc = (y_pred.round() == y_batch).float().mean()\n",
    "            bar.set_postfix(\n",
    "              loss=float(loss),\n",
    "              acc=float(acc)\n",
    "            )\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    acc = (y_pred.round() == y_test).float().mean()\n",
    "    acc = float(acc)\n",
    "    history.append(acc)\n",
    "    print(\"Accuracy: \" + str(acc))\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # restore model and return best accuracy\n",
    "model.load_state_dict(best_weights)\n",
    "# restore model and return best accuracy\n",
    "\n",
    "y_pred_nn = model(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_nn.round().detach().numpy())\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "print(cm)\n",
    "print(\"Background fake rate: \" + str(cm[0,1]/(cm[0,1] + cm[0,0])))\n",
    "print(\"Signal accuracy: \" + str(cm[1,1]/(cm[1,1] + cm[1,0])))\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "y_pred_all_mass_nn = model(X_test_all_mass)\n",
    "pred_mask = y_pred_all_mass_nn.round().detach().numpy().reshape(-1)\n",
    "true_mask = y_test_all_mass\n",
    "\n",
    "x_test_signal_mass = X_test_keep_mass[:,4][(true_mask == 1) & (pred_mask == 1)]\n",
    "x_test_background_mass = X_test_keep_mass[:,4][(true_mask == 0) & (pred_mask == 1)]\n",
    "\n",
    "plt.hist(x_test_signal_mass,bins=20,range=(0,2000),density=True,\n",
    "         label='Signal',fill=False,edgecolor='red',linewidth=3,histtype='step');\n",
    "plt.hist(x_test_background_mass,bins=20,range=(0,2000),density=True,\n",
    "         label='Background',fill=False,edgecolor='blue',linewidth=3,histtype='step');\n",
    "\n",
    "plt.xlabel(r'Mass (GeV/c)',fontsize=14)\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9006ab3d-24c9-42c9-b12b-2e95a0f6c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_nn.detach().numpy())\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='NN Classification')\n",
    "\n",
    "display.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51876e9f-07f5-4942-a0a3-6ac8249eb5a3",
   "metadata": {},
   "source": [
    "## Take home message and exercises\n",
    "\n",
    "Using these two examples, we have introduced two major tasks in HEP where ML can help: regression and classification. Both examples are supervised approaches. We did not introduce un-supervised approaches due to time limits but these two examples should have already taught you the basic SW skills to go further. \n",
    "\n",
    "As mentioned several times during this course, ML in HEP is promising but we should not abandon our physics instincts and knowledge. In particular, for experimentalists, how to ensure good control over the performance in data is very crucial. Not all problems in HEP are worth solving using ML at the moment, as in certain areas classic methods still retain their power. However, we should all be open-minded and dare to change. \n",
    "\n",
    "Here is a small project for you to exercise more. You will need to acquire some SW knowledge that has not been introduced in this course, but you should know how to get them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af66d7d7-fe88-4dec-8b6c-9c6ead46b6c3",
   "metadata": {},
   "source": [
    "### Comprehensive exercise\n",
    "\n",
    "**Goal**: design a classifier that:\n",
    "\n",
    "**1**: Works for the various mass pointes provided\n",
    "\n",
    "**2**: Does not sculpt the background invariant mass distribution significantly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c3ad5-d2e0-4009-8ba4-e6e3e9dbd849",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
