{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfff020-287a-47f4-9420-a808cb283740",
   "metadata": {},
   "source": [
    "# AI Application in HEP Tutorial (Supervised)\n",
    "\n",
    "In this tutorial we will go through a series of popular supervised ML methods to solve a physics problem with an exact answer which is the calculation of the invatiant mass of two jets. \n",
    "\n",
    "As the word \"supervised\" suggests, we do need to know the correct answers to train the algorithms. In this example, we know how to calculate the invariant mass of the two jets, i.e., we know the real model. But we will see that various ML methods may offer acceptable alternative models, using the jet kinematic info. In real research, the actual goal is much more compicated than obtaining the invariant mass of the two jets, and we can not access the true model easily.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bb8c00-7d0b-45b9-8bba-2e503e024f4a",
   "metadata": {},
   "source": [
    "## Data Acquisation\n",
    "\n",
    "Before diving into the algorithmic part, let us spend sometimes on the data acquisation, or data mining. There are well established ways of storing the data, in both of the ML world and HEP, so it is important to get used to the shared methods. \n",
    "\n",
    "If you have read about ML, you should know that python is the most popular language. Creating a python-based workflow will save you so much time in the long run. In this section we will consider a ROOT (https://root.cern.ch/) output from Delphes (https://github.com/delphes/delphes), a fast detector simulation SW, using uproot (https://github.com/scikit-hep/uproot5) that is pure python. \n",
    "\n",
    "Another important library is called awkward (https://awkward-array.org/doc/main/index.html), designed to handle variable-sized and nested data, which is exactly what event records from HEP experiments usually have.\n",
    "\n",
    "All the packages are open source, so you can get them for free. Their websites also offer comprehensive tutorials and documentations:\n",
    "\n",
    "pandas (https://pandas.pydata.org/) and numpy (https://numpy.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7077f89e-317b-47ea-8eee-3867ccb5cb91",
   "metadata": {},
   "source": [
    "### Import all necessary libraries\n",
    "\n",
    "It is better to use a package manager. You can pick your favourite but in this tutorial we assume you are using conda (https://anaconda.org/anaconda/conda)\n",
    "\n",
    "Once these libraries are installed and linked correctly, you should be able to execute the following lines without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a9e69e-29a6-42f8-bc09-a50ac7831f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import *\n",
    "from tqdm import tqdm\n",
    "import uproot as r\n",
    "import awkward as ak\n",
    "import vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7163a6a7-44fb-4ba4-9a38-09948b470e41",
   "metadata": {},
   "source": [
    "### Load the input data and extract the information\n",
    "\n",
    "We have prepared a test Delphes output for you, which is a heavy particle (Z') decaying to two jets. The heavy particle mass is set to 1 TeV. So we are suppose to see a peak around 1 TeV when checking the invariant mass distribution of the jets. Root files store the data in TTrees. We will extract the jet info and save the 4-vectors in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c40e6c-dbf0-4cd7-b696-5a9469e31c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following syntax to read a delphes root file and parse the branches to arrays\n",
    "# You might want to open the test file using ROOT to see the structure. \n",
    "# What is the tree name? How are all the branches defined?\n",
    "\n",
    "f = r.open(\"./data/delphes_zprime_1TeV.root\")\n",
    "delphes_tree = f[\"Delphes;1\"]\n",
    "\n",
    "pt = delphes_tree[\"Jet.PT\"].array()\n",
    "eta = delphes_tree[\"Jet.Eta\"].array()\n",
    "phi = delphes_tree[\"Jet.Phi\"].array()\n",
    "m = delphes_tree[\"Jet.Mass\"].array()\n",
    "\n",
    "# Here we use the vector (https://github.com/scikit-hep/vector) library\n",
    "# It allows us to use its implemented Lorentz vector functions.\n",
    "\n",
    "vector.register_awkward()\n",
    "jet_vec = ak.zip({\n",
    "  \"pt\": pt,\n",
    "  \"phi\": phi,\n",
    "  \"eta\": eta,\n",
    "  \"mass\": m,\n",
    "},with_name=\"Momentum4D\")                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51231e58-1c37-4f32-b787-f9db182c90c4",
   "metadata": {},
   "source": [
    "Now let's check the constructed jet_vec object. We want to select events with at least two jets in order to get the invariant mass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25e001-6fa7-4de4-9b05-de0fc165d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(jet_vec))\n",
    "\n",
    "# You need to get familiar with the array operations. To select elements satisfying certain \n",
    "# criteria, you should create a mask, which is an array consisting of booleans [true, false, false, ..]\n",
    "# and then apply the mask to an array: b = a[mask]\n",
    "\n",
    "jet_vec_select = jet_vec[ak.num(jet_vec) >= 2] # https://awkward-array.org/doc/main/reference/generated/ak.num.html\n",
    "print(len(jet_vec_select))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5940d0-c10d-4b26-815a-47849c55156d",
   "metadata": {},
   "source": [
    "Then let us get the leading and sub-leading jets. We can assign them to individual Lorentz vectors, after which all Lorentz vector operations supported by the vector library can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d87f32c-f6bd-4769-b6a8-c4e7e49c2782",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_jet = jet_vec_select[:,0] # https://stackoverflow.com/questions/16815928/what-does-mean-on-numpy-arrays\n",
    "sublead_jet = jet_vec_select[:,1]\n",
    "\n",
    "dijet = lead_jet + sublead_jet\n",
    "dijet_mass = dijet.mass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da04d09-912b-4d06-a139-6b70820ca1a2",
   "metadata": {},
   "source": [
    "### Visualizing the data\n",
    "\n",
    "It is important, all the time, to first check the input data before diving deep into ML. Visualizing data using plotting suite is highly recommended. We often use matplotlib (https://matplotlib.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ad57c-db7a-407b-a385-b9a0ea577344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "# The matplotlib hist function takes 1-D numpy arrays as the input \n",
    "# so a step converting awkward array to 1-D numpy is needed\n",
    "\n",
    "print(pt.type)\n",
    "print(ak.flatten(pt).type)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(ak.flatten(pt),bins=20,range=(0,1000)); #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "plt.xlabel(r'Jet $p_T$ (GeV/c)',fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pt[ak.num(pt) >= 1][:,0],bins=20,range=(0,1000));\n",
    "plt.xlabel(r'Leading Jet $p_T$ (GeV/c)',fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pt[ak.num(pt) >= 2][:,1],bins=20,range=(0,1000));\n",
    "plt.xlabel(r'Sub-leading Jet $p_T$ (GeV/c)',fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(dijet_mass,bins=20,range=(0,2000));\n",
    "plt.xlabel(r'Mass  (GeV/c)',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c22be-cedb-418f-ad04-5871c2a9bae8",
   "metadata": {},
   "source": [
    "**Homework**: \n",
    "\n",
    "**1**: Try to change the plotting style and see its impacts. How to plot normalized histograms? How to plot multiple histograms together (try to plot leading and sub-leading jet pT on th sample plot with legends).\n",
    "\n",
    "**2**: Try to create similar plots for other variables such as jet $\\eta$ and $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60843d98-6498-4155-9c8a-12cf6cf52d54",
   "metadata": {},
   "source": [
    "### Save datasets for ML\n",
    "\n",
    "After checking the data and confirming that it is reasonable, we can save the dataset into a csv file.\n",
    "\n",
    "**Qusestion**: how do we want to save the data? If we want a csv file, what are the rows and columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278ffa1-93e1-4b5b-8f2e-243fe2076f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to again operate numpy arrays. First we convert the awkward array to \n",
    "# numpy arrays via the to_numpy function. In addition, we need to change the shape of the array \n",
    "# so that each quantity forms a 2-D array.\n",
    "\n",
    "print(ak.to_numpy(lead_jet.pt).shape)\n",
    "print(ak.to_numpy(lead_jet.pt).reshape(1,-1).shape)\n",
    "\n",
    "# As you can see, it is converted into one single row\n",
    "\n",
    "lead_jet_pt = ak.to_numpy(lead_jet.pt).reshape(1,-1)\n",
    "lead_jet_eta = ak.to_numpy(lead_jet.eta).reshape(1,-1)\n",
    "lead_jet_phi = ak.to_numpy(lead_jet.phi).reshape(1,-1)\n",
    "lead_jet_e = ak.to_numpy(lead_jet.e).reshape(1,-1)\n",
    "sublead_jet_pt = ak.to_numpy(sublead_jet.pt).reshape(1,-1)\n",
    "sublead_jet_eta = ak.to_numpy(sublead_jet.eta).reshape(1,-1)\n",
    "sublead_jet_phi = ak.to_numpy(sublead_jet.phi).reshape(1,-1)\n",
    "sublead_jet_e = ak.to_numpy(sublead_jet.e).reshape(1,-1)\n",
    "dijet_mass = ak.to_numpy(dijet_mass).reshape(1,-1)\n",
    "\n",
    "# Now lets use the row_stack function to stack all the rows together\n",
    "\n",
    "dataset = np.row_stack((lead_jet_pt, lead_jet_eta, lead_jet_phi, \n",
    "                          lead_jet_e, sublead_jet_pt, sublead_jet_eta, \n",
    "                          sublead_jet_phi, sublead_jet_e, dijet_mass))\n",
    "print(dataset.shape)\n",
    "\n",
    "# We (most libraries) treate each row as one record, and each column as a feature. \n",
    "# Let's transpose the above dataset to facilitate this convention.\n",
    "\n",
    "print(np.transpose(dataset).shape)\n",
    "\n",
    "dataset_df = pd.DataFrame(np.transpose(dataset), columns=['jet1_pt', 'jet1_eta', 'jet1_phi', 'jet1_e', \n",
    "                                                          'jet2_pt', 'jet2_eta', 'jet2_phi', 'jet2_e', 'mass']) \n",
    "\n",
    "dataset_df.to_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba1b73b-853c-488a-844f-ac559f4bf733",
   "metadata": {},
   "source": [
    "Now you should have saved a dataset.csv file. You should be able to check the content easily. See whether it is as expected.\n",
    "\n",
    "**Homework**: Let's do some exercise using numpy. How to create columns first and then use np.column_stack?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07984cb-51a1-4090-a859-bcd8ba7d600b",
   "metadata": {},
   "source": [
    "## Linear Model\n",
    "\n",
    "Now we are ready to train our models. Let's start with a linear regeression model. \n",
    "\n",
    "The scikit-learn library (https://scikit-learn.org/stable/index.html) is an excellent choice for linear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c2eee-6e17-437d-b77d-0756a5a4ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# pandas can read the csv files and it has many convenient functions.\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df.info()\n",
    "print(df.head())\n",
    "print(\"Drop unamed:\")\n",
    "df.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e1b6d9-a1bb-4389-ba94-bc2d61833b4f",
   "metadata": {},
   "source": [
    "### Prepare the training and testing datasets\n",
    "\n",
    "As we have mentioned in the class, it is crucial to have orthogonal training and testing datasets. But first we need to make sure what is the training target. In our case, the invariant mass calculated via the Lorentz vector formula is the target, while the rest is the feature.\n",
    "\n",
    "It is also a very good practice to check again the dataset via some plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f5815-2252-4f13-b979-17150c867897",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['mass'].copy()\n",
    "X = df.drop(['mass'], axis=1).to_numpy()\n",
    "\n",
    "print(type(X))\n",
    "print(X.shape)\n",
    "\n",
    "# Let's plot the eta-phi map\n",
    "\n",
    "plt.scatter(X[:,1],X[:,2])\n",
    "plt.xlabel(\"Eta\")\n",
    "plt.ylabel(\"Phi\")\n",
    "plt.show()\n",
    "\n",
    "print(type(y))\n",
    "print(y.shape)\n",
    "\n",
    "# Let's plot the invariant mass\n",
    "\n",
    "plt.hist(y,bins=20,range=(0,2000));\n",
    "plt.xlabel(r'Mass  (GeV/c)',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73cab3-0628-4cb5-a80f-c4f6dfc24f48",
   "metadata": {},
   "source": [
    "The dataset splitting and the training are very straightforward. They can be done by a few lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8c006-1d93-4a00-82d5-7a106053c66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,\n",
    "                                                    random_state=42) # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "# Call the linear regression model. It is a class with various useful functions.\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training dataset.\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Test the model using the test dataset.\n",
    "pred_X = regr.predict(X_test)\n",
    "\n",
    "# Check the performance\n",
    "plt.scatter(y_test, pred_X)\n",
    "plt.xlabel(\"Truth Mass\")\n",
    "plt.ylabel(\"Predicted Mass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d6e9b8-3224-4dba-860c-fad9722fd627",
   "metadata": {},
   "source": [
    "Clearly the linear model is not sufficient. We knwo the invariant mass of the two jets are from non-linear combinations of the 4-vector. What if we use a non-linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268c6c4-c9af-42f1-be47-9c69d7e894bf",
   "metadata": {},
   "source": [
    "## Boosted Regression Tree\n",
    "\n",
    "Let's now consider a boosted regression tree, which is a non-linear model. A tree is very illustrative and in most of the cases it can have competitive performance. The xgboost library (https://xgboost.readthedocs.io/en/latest/index.html) is used by the majority of the community. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c882e83-760e-4cd5-94da-8777c15b1b6a",
   "metadata": {},
   "source": [
    "### Checking the correlations\n",
    "\n",
    "Non-linear models attempt to solve the problem in a high-dimension space. It is even more critical to understand the input data, in particular the correlations between them, before going further. We should get some tuition and build up some expectation for the non-linear model.\n",
    "\n",
    "**Question**: what kinematic variables are more important for the invariant mass calculation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890e582-536a-4385-aa20-b5faa459b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas can falculate the covariance matrix for us and the seaborn library can make \n",
    "# very nice heatmaps (https://seaborn.pydata.org/).\n",
    "\n",
    "X_and_y = df\n",
    "X_and_y.corr()\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(X_and_y.corr(), cmap = \"YlGnBu\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36046b0b-d24f-4a9c-ab29-480b88eff287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also rank the correlations\n",
    "\n",
    "X_and_y.corr()['mass'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ae87e-68fb-405c-80ca-b069a37bbc4c",
   "metadata": {},
   "source": [
    "### Standardization and normalization\n",
    "\n",
    "When considering data with various features, it is good to have a sense of the scale of each feature. If one or a few of them are way larger than the rest, the minimization step may struggle. It is usually a good practice to standardize or normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759277f-1d89-45f1-8b9d-eb6f7a24e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# For the target we can simply do it by hand\n",
    "y_train = y_train/1000.0\n",
    "y_test = y_test/1000.0\n",
    "y = y/1000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259dd0e-51b5-46c7-88ce-cf5278458dd6",
   "metadata": {},
   "source": [
    "### Train a boost regression tree using xgboost\n",
    "\n",
    "Similar as the linear model, we can apply a boost regression tree out of the box using xgboost. The model has quite some parameters. Please have a read through here: https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "\n",
    "**Question**: which parameters have larger impact on the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032eedb4-a846-4d3d-8689-8a5140b5fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# Instatiate a XGBRegressor\n",
    "xgb_reg = xgb.XGBRegressor(random_state=123)\n",
    "\n",
    "# Inspect the parameters\n",
    "xgb_reg.get_params()\n",
    "\n",
    "## Set the number of trees to 10 (gradient boost tree is called by default)\n",
    "xgb_reg.set_params(n_estimators=10)\n",
    "\n",
    "# Fit it to the training set\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the values of the test set\n",
    "preds = xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21f550d-c96a-4b53-a54e-4acb07df6cf8",
   "metadata": {},
   "source": [
    "### Evaluate the performance\n",
    "\n",
    "The metric used to evaluate the performance is very similar for all regression tasks. Since we would like to check the performance using various model parameters, let's write up a module that gives us nice performance plots automatically. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ffec5-ff0a-434b-b842-864c16c24098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Calculate the relative errors\n",
    "\n",
    "# remember when you need to perform the same task over and over again, write a function.\n",
    "def checkPerf(preds, y_test):\n",
    "\n",
    "    rel_err = (preds  - y_test)/y_test\n",
    "\n",
    "    # Print the baseline accuracy\n",
    "    print(\"Mean relative error:\", np.mean(rel_err*100))\n",
    "\n",
    "    plt.scatter(y_test, preds)\n",
    "    plt.xlabel(\"True Mass\")\n",
    "    plt.ylabel(\"Predicted Mass\")\n",
    "    plt.show()\n",
    "\n",
    "    bins = np.arange(-100, 100, 5)\n",
    "    plt.hist(rel_err*100, bins)\n",
    "    #plt.yscale('log')\n",
    "    plt.xlabel(\"Relative Error (%)\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.show()\n",
    "\n",
    "    sns.boxplot(rel_err*100)\n",
    "\n",
    "checkPerf(preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c957e019-0580-446d-80f9-34cc16a346fe",
   "metadata": {},
   "source": [
    "### Tree visualization\n",
    "\n",
    "As mentioned before, one great advantage of the boost tree is its interpretability. Now let's see how to use those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6b8d22-698a-4883-b523-5109d088355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8)\n",
    "\n",
    "xgb.plot_importance(xgb_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6255dd35-24ca-497e-b8a3-a481b89cfddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gain instead of weight\n",
    "xgb.plot_importance(xgb_reg, importance_type=\"gain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133d433-b4d7-4690-9a7d-45912e533a81",
   "metadata": {},
   "source": [
    "**Question**: are they as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6a682-9bda-49c5-8fe2-d1cd839a0745",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (20.0, 8)\n",
    "\n",
    "# Plot the first tree\n",
    "xgb.plot_tree(xgb_reg, num_trees=0)\n",
    "\n",
    "format = 'svg' \n",
    "\n",
    "image = xgb.to_graphviz(xgb_reg)\n",
    "\n",
    "#Set a different dpi (work only if format == 'png')\n",
    "image.graph_attr = {'dpi':'400'}\n",
    "\n",
    "image.render('bdt_regressor', format = format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ef6bc-b548-446f-9b5d-edcb7e1dbfa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb.plot_tree(xgb_reg, num_trees=9, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a131bea-1fc7-4313-aa8f-847d1cc554e0",
   "metadata": {},
   "source": [
    "**Homework**: \n",
    "\n",
    "**1**. Try various model parameters and check the performance (how to compare the performance of various models? how to plot them together?) \n",
    "\n",
    "**2**. Compare the linear model withyour best xgboost model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae5b89b-872a-4609-bf21-f3d7da6cfdfa",
   "metadata": {},
   "source": [
    "## Deep Neutral Network\n",
    "\n",
    "We have seen that a non-linear model like boost regression tree obtains better results. However, the performance is far from ideal. Now let's go deeper. A NN has much increased non-linearity to capture deeper underlying relationships. We will use the pytorch library to achieve this (https://pytorch.org/).\n",
    "\n",
    "We will skip the dataloading and splitting steps as we just have to repeat what we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb934cf-5a69-4e7e-b97a-19c947a56cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.optim as optim\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "y = df['mass'].copy()\n",
    "X = df.drop(['mass'], axis=1).to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle = True)\n",
    "\n",
    "X_train_raw = X_train\n",
    "X_test_raw = X_test\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "\n",
    "# Why do we need this step?\n",
    "y_test = y_test/1000.0\n",
    "y_train = y_train/1000.0\n",
    "y = y/1000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71338ae9-232d-4027-824f-5dff323587bf",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "Buidling a NN in pytorch is fairly easy, and you only need to know the syntax and the NN structure. In this example, we will build a vanilla feed forward NN, using fully connected layers and ReLU activation. For more layers, please check: https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "As you will see, it is like building a Lego model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb0cb4-850d-4fe5-bd1a-cf8a821dbffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetworkRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # The number of input nodes matches the number of total features\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            # The fully connected layers have to have compatible number of nodes.\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "        \n",
    "# loss function and optimizer\n",
    "model = NeuralNetworkRegression().to(device)\n",
    "loss_fn = nn.MSELoss()  # mean square error. It is not cross-entropy for our regression task\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87894d69-96b0-45b3-8ee9-ef0532d48d20",
   "metadata": {},
   "source": [
    "**Question**: Can you describe the above network using youe own language?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0de52e-0441-43fb-a540-2448f24b7647",
   "metadata": {},
   "source": [
    "### Change the input format to tensor\n",
    "\n",
    "pytorch uses a tensor object so that we need to transform the input data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f14fba-6417-4d89-b92c-88781762d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "print(X_train.type)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c811c9d-42ab-4f89-82b0-5a47a02eb6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import copy\n",
    "\n",
    "# training parameters\n",
    "n_epochs = 200   # number of epochs to run\n",
    "batch_size = 10  # size of each batch\n",
    "batch_start = torch.arange(0, len(X_train), batch_size) #find the location of each sample for each batch\n",
    " \n",
    "# Hold the best model\n",
    "best_mse = np.inf   # init to infinity, so the minimization will start decreasing the mse\n",
    "best_weights = None\n",
    "history = []\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch \" + str(epoch))\n",
    "    #Set the model to training mode. This is needed as some times the layers will behave differently in training \n",
    "    #and testing. Though in our exmaples both the RELU and Linear functions behace the same, it is a good practice to set the \n",
    "    #Model to training mode before it starts training. \n",
    "    model.train()\n",
    "    #Here we are using tqdm (https://tqdm.github.io/) to monitor the process but the effects can only bee seen in a terminal. \n",
    "    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for start in bar:\n",
    "            # take a batch\n",
    "            X_batch = X_train[start:start+batch_size].to(device)\n",
    "            y_batch = y_train[start:start+batch_size].to(device)\n",
    "            # forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            # calculate the loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # backward pass\n",
    "            # here we set zero_grad() why?\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update weights using one single step.\n",
    "            optimizer.step()\n",
    "            # print progress\n",
    "        bar.set_postfix(mse=float(loss))      \n",
    "        # evaluate accuracy at end of each epoch. We set the model to testing model for the same reasons as above.\n",
    "    model.eval()\n",
    "    y_pred = model(X_test).to(device)\n",
    "    mse = loss_fn(y_pred, y_test)\n",
    "    mse = float(mse)\n",
    "    history.append(mse)\n",
    "    print(\"MSE: \" + str(mse))\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    " \n",
    "# restore model and return best accuracy\n",
    "model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c8ceb-1904-4f77-80a5-6aae34394140",
   "metadata": {},
   "source": [
    "**Question**: Can you explain each step in the optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc1400f-95b5-4b1f-9bbb-5ca6ec87e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n",
    "\n",
    "# load model\n",
    "model = NeuralNetworkRegression().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "start = 0\n",
    "end = len(X_test_raw)\n",
    "\n",
    "# let's plot the predicted value vs the true value.\n",
    "with torch.no_grad():\n",
    "    X_sample = X_test_raw[start: end]\n",
    "    X_sample = scaler.transform(X_sample)\n",
    "    X_sample = torch.tensor(X_sample, dtype=torch.float32).to(device)\n",
    "    y_pred = model(X_sample)\n",
    "    y_pred_np = y_pred.detach().cpu().numpy()\n",
    "    y_test_np = y_test.detach().cpu().numpy()   \n",
    "\n",
    "    plt.scatter(y_test_np[start: end].reshape(1,-1), y_pred_np.reshape(1,-1))\n",
    "    plt.xlabel(\"True Mass\")\n",
    "    plt.ylabel(\"Predicted Mass\")\n",
    "    plt.show()\n",
    "\n",
    "    rel_err = (y_pred_np - y_test_np[start: end]) * 100/y_test_np[start: end]\n",
    "    bins = np.arange(-10, 10, 1)\n",
    "    plt.hist(rel_err, bins)\n",
    "    #plt.yscale('log')\n",
    "    plt.xlabel(\"Relative Error (%)\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"The mean relative error is: \")\n",
    "    print(str(np.mean(rel_err)) + \"%\")\n",
    "\n",
    "    import seaborn as sns\n",
    "\n",
    "    sns.boxplot(rel_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f4faff-fffb-487f-b3e8-2ab4f3f633a4",
   "metadata": {},
   "source": [
    "**Homework**:\n",
    "\n",
    "**1**. How to reduce the NN to a linear model here? \n",
    "\n",
    "**2**. Try different model structures and see how the performance changes. Try to also change the fractions of training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf31e6-589a-47f0-bb55-4867ca7ee7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
