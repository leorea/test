{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfff020-287a-47f4-9420-a808cb283740",
   "metadata": {},
   "source": [
    "# Python in Data Analysis\n",
    "\n",
    "iSTEP 2025, instructor Bingxuan Liu (SYSU, liubx28@mail.sysu.edu.cn)\n",
    "\n",
    "In this tutorial, we will learn how to use Python to perform basic data analysis tasks in HEP experiments. As you will see, there is an increasing trend of utilising open-source, community-maintained tools/packages in our field. This path ensures better knowledge transfer and equips you with the essential skill-set for a broad range of applications, not necessarily in HEP only.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec61422-207f-46b8-ab4a-12d8e94e5dd7",
   "metadata": {},
   "source": [
    "## Data Structure \n",
    "\n",
    "Like data analysis in general, the data structure is a vital component. If we think about the data we usually analyse in particle physics, what features do they have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a0cfb-858c-48cd-98f7-fcf4d2e232a4",
   "metadata": {},
   "source": [
    "### An Event\n",
    "\n",
    "In particle physics, we consider either collision data or simulation events. An event contains the products of a given physics process (or a bunch of processes in hadron colliders, which has pile-up interactions), and the products are experiment-specific. The contents, though, depend on the data-processing stages. The data-processing pipeline collects raw detector inputs (pure electronic signals), and covert them to a format needed by the downstream steps. The chain can be quite long depending on the complexity of the experiment. We will not go into detail in this tutorial. If you are interested, you could find literatures by looking up this key word: **Event Data Model (EDM)**\n",
    "\n",
    "This tutorial uses data format that is ready to be analysed, which means we have direct access to the physics objects, such as jet, leptons and photons. Sometimes, even higher level objects such as top candidates, W/Z candidates and Higgs candidates are available. Now, let's ask ourselves the following question: \n",
    "\n",
    "<center><b>How should we store everything in a coherent way?</b></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f56cd7-df4e-4e4a-ba99-57f3ed212f1f",
   "metadata": {},
   "source": [
    "### Event Structure\n",
    "\n",
    "Let's say we have an event from collision data collected by ATLAS, what does it usually look like?\n",
    "\n",
    "|Event   | Jet      | Muon    | Electron | Photon  |\n",
    "|--------| -------- | ------- | -------- | ------- |\n",
    "|        | Jet1     | Muon1   | Electron1| Photon1 | \n",
    "| Event1 | Jet2     | --      | Electron2| Photon2 |\n",
    "|        | Jet3     | --      | --       | --      |\n",
    "|        | Jet1     | Muon1   | Electron1| Photon1 | \n",
    "| Event2 | Jet2     | Muon2   | --       | --      |\n",
    "|        | Jet3     | --      | --       | --      |\n",
    "|        | Jet4     | --      | --       | --      |\n",
    "|  ...   |  ...     |  ...    |  ...     |  ...    | \n",
    "\n",
    "And for each object such as a jet, we would like to know its $p_{\\mathrm{T}}$, $\\eta$, $\\phi$ and other important properties such as the hadronic energy fraction, etc. For simplicity, let's only consider the 4-vectors of the physics objects. Therefore, the full table becomesï¼š\n",
    "\n",
    "|Event   | Jet      | Muon    | Electron | Photon  |\n",
    "|--------| -------- | ------- | -------- | ------- |\n",
    "|        | Jet1 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$] | Muon1 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$] | Electron1 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$]| Photon1 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$]| \n",
    "| Event1 | Jet2 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$] | --      | Electron2 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$]| Photon2 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$] |\n",
    "|        | Jet3 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$] | --      | --       | --      |\n",
    "|        | Jet1 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$] | Muon1 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$] | Electron1 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$]| Photon1 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$] | \n",
    "| Event2 | Jet2 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$] | Muon2 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$] | --       | --      |\n",
    "|        | Jet3 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$] | --      | --       | --      |\n",
    "|        | Jet4 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$] | --      | --       | --      |\n",
    "|  ...   |  ...     |  ...    |  ...     |  ...    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f7ecbc-5fce-4154-8b06-f044d9e51448",
   "metadata": {},
   "source": [
    "### Data Structure\n",
    "\n",
    "The data structure should have the ability to locate every single variable in data, for example, $p_{\\mathrm{T}}$ of jet1 in event2. The simplest structure is very common and you must have seen it:\n",
    "\n",
    "|Event   | Jet1 $p_{\\mathrm{T}}$ | Jet1 $\\eta$ | Jet1 $\\phi$ | Jet1 $m$  | Jet2 $p_{\\mathrm{T}}$ | Jet2 $\\eta$ | Jet2 $\\phi$ | Jet2 $m$  | ...|\n",
    "|--------| -------- | ------- | -------- | ------- | -------- | ------- | -------- | ------- |------- |\n",
    "|Event1   | 100 GeV | 1.2 | -1.5 | 15 GeV | 80 GeV | 0.2 | 1.4 | 12 GeV | ...| \n",
    "|Event2   | 120 GeV | 0.9 | 2.1 | 8 GeV | NaN | NaN | NaN | NaN | ...| \n",
    "|Event3   | ... | ... | ... | ... | ... | ... | ... | ... | ...|\n",
    "\n",
    "Yep, it is just the Excel sheets we have all used (or been tortured by). It is also called a CSV format (comma separated variables). The syntax is straightforward, as indicated by the name, that each variable is separated by commas. This is arguably the most adopted data format in the area of data analysis, so we encourage you to get familiar with it, and use it as the starting point. This also allows you to easily use data produced in other disciplines. \n",
    "\n",
    "Another more advanced data structure groups similar variables, such as jet $p_\\mathrm{T}$. In this case, each event contains a field (column) called \"jet $p_\\mathrm{T}$\", etc. If we present this structure in the above fashion, we will get:\n",
    "\n",
    "|Event   | Jet $p_\\mathrm{T}$ | Jet $\\eta$ | Jet $\\phi$ | Jet $m$  | ... |\n",
    "|--------| -------- | ------- | -------- | ------- | ------- |\n",
    "| Event1 | 100 GeV  | 1.2   | -1.5 | 15 GeV | ... |\n",
    "|        | 80 GeV   | 0.2   | 1.4  | 12 GeV | ... |\n",
    "| Event2 | 120 GeV  | 0.9   | 2.1  | 8 GeV  | ... |\n",
    "|  ...   |  ...     |  ...  |  ... |  ...   | ... |\n",
    "\n",
    "This is the so-called **\"flat n-Tuple\"**, which means each field (column) is a variable, instead of a physics object as the first table. If the data contains physics objects such as jets directly, it is the **Analysis Object Data (AOD)**:\n",
    "\n",
    "|Event   | Jet      | ...    | \n",
    "|--------| -------- | ------- | \n",
    "| Event1 | Jet1 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$] | ... | \n",
    "|        | Jet2 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$] | ... | \n",
    "| Event2 | Jet1 [$p_{\\mathrm{T}}$, $\\eta$, $\\phi$, $m$] | ... | \n",
    "|  ...   |  ...     |  ...    |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107307be-c8eb-4340-850c-6d4802d0dd28",
   "metadata": {},
   "source": [
    "## Python\n",
    "\n",
    "How is everything related with Python? Well, every format mentioned above can be seen as arrays. The simplest CSV format is equivalent to a 2D array. Let's see how to handle those first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2dda33-f591-4db6-bec9-bf190302d43d",
   "metadata": {},
   "source": [
    "### Numpy and Pandas\n",
    "\n",
    "numpy (https://numpy.org/) is an open-source package specialised in array and matrix operations. You will find it extremely popular in data analysis and machine learning, because of its design goal. pandas (https://pandas.pydata.org/) is a python-based data analysis library, with a lot of nice built-in functionalities. This section we will go through some basic usages of these two packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d9841-21ba-4d1c-b19f-52659c6fd9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e376648-bff1-4b81-92d1-720f4a0966f0",
   "metadata": {},
   "source": [
    "Let's create a numpy array by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100d88e4-5ee2-45d0-8447-d8e8141686a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_pt1 = np.array([100, 120],'d')\n",
    "print(jet_pt1)\n",
    "print(jet_pt1.size)\n",
    "print(jet_pt1.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b4488-8eae-4a2f-976b-150593fd2444",
   "metadata": {},
   "source": [
    "We have created an array with two elements, corresponding to the leading jet (jet1) $p_{\\mathrm{T}}$. Now lets create a longer one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e82e6-bf09-40a6-98d9-cad0a5d7e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_pt1 = np.array([100, 120, 80, 70, 150, 200, 160, 250],'d')\n",
    "print(jet_pt1)\n",
    "print(jet_pt1.size)\n",
    "print(jet_pt1.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e53150-9b52-4142-a2b1-3bbbbf3aa6d2",
   "metadata": {},
   "source": [
    "In data analysis, we need to select data satisfying a given condition. This can be done via array operations. For instance, lets pick out the events with leading jet $p_{\\mathrm{T}}$ larger than 100 GeV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3b926-aa0a-4517-aa5c-515b0d85d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = jet_pt1 > 100\n",
    "print(selection)\n",
    "print(jet_pt1[selection])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f968dd11-2606-4844-9c51-06fe052b1cda",
   "metadata": {},
   "source": [
    "As you can see, the logic operations using arrays will give you a mask, and you can use this mask to select the data you want. Lets see a more complicated case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf759af-fa96-4c11-abb7-b12a857922d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_pt1 = np.array([100, 120, 80, 70, 150, 200, 160, 250],'d')\n",
    "jet_eta1 = np.array([2.1, 1.5, -1.1, -2.5, 0.8, -0.4, -0.6, 0.9],'d')\n",
    "selection = (np.abs(jet_eta1) < 2) & (jet_pt1 > 100)\n",
    "print(selection)\n",
    "print(jet_pt1[selection])\n",
    "print(jet_eta1[selection])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dbd621-b6e3-44ff-b930-9aac4a2f8f59",
   "metadata": {},
   "source": [
    "In the above example, we applied a selection that requires the leading jet $p_{\\mathrm{T}}$ to be larger than 100 GeV and the absolute $\\eta$ to be less than 2. You can construct any selections you want, but some complicated logics may need quite some thinkings. It is much more convenient than the traditional C++ way:\n",
    "\n",
    "```\n",
    "using namespace std;\n",
    "\n",
    "vector<float> selected_jet_pt1;\n",
    "\n",
    "vector<float> selected_jet_eta1;\n",
    "\n",
    "for (uint i; i < jet_pt1.size(); i++) {\n",
    "\n",
    "    if (jet_pt1.at(i) > 100 && abs(jet_eta1.at(i)) < 2) {\n",
    "    \n",
    "        selected_jet_pt1.push_back(jet_pt1.at(i));\n",
    "        \n",
    "        selected_jet_eta1.push_back(jet_eta1.at(i));\n",
    "        \n",
    "    }\n",
    "    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c859f-4807-4f07-8844-f26f6d9f4d76",
   "metadata": {},
   "source": [
    "You need to gradually get familiar with the numpy array operations. With the help of internet and AI, you can find solutions to almost all your numpy related problems quickly. Experience comes with practice.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ed903-94d2-4c06-a488-ddb652c0299f",
   "metadata": {},
   "source": [
    "pandas, on the other hand, offers higher level functions to handle datasets. You will find it very useful when you have an input SCV file. It can also be used to create CSV files. Let's first create one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8389f-5f7c-4396-bf3f-1e499c037f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_pt1 = np.array([100, 120, 80, 70, 150, 200, 160, 250],'d')\n",
    "df = pd.DataFrame(jet_pt1, columns=['jet_pt1']) \n",
    "df.to_csv(\"dummy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b34147e-8e24-46ca-b88b-a37a26344715",
   "metadata": {},
   "source": [
    "Now you should see a dummy.csv file in the current working directory. Open it and see what it contains. You may realise instantly that there is a column without a header. Next, let's load this file and do some operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af57be-d0c5-408c-8829-b3375ad79b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df = pd.read_csv('dummy.csv')\n",
    "print(read_df)\n",
    "print(read_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c07830-aad6-4f1a-b792-122c7c104519",
   "metadata": {},
   "source": [
    "There is a column with a header called \"Unnamed: 0\", which is not needed. Let's drop it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c80c0-4d14-4c86-bc4a-c845fd77a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "print(read_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c309174-ecc7-4d16-b69a-165a2e927d7b",
   "metadata": {},
   "source": [
    "We can also add a new column. This is a very useful function. When trying ML, in particular unsupervised ML techniques, we need to label the dataset. Here if we assume the data comes from background, and we want to add a new column to represent the label, how to achieve this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391ea71-cfd3-49aa-b8b8-04547d4eed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df['label'] = 'background'\n",
    "print(read_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b09104-9ab0-45cf-bca6-b3006f2a471b",
   "metadata": {},
   "source": [
    "You can always construct new variables to be added to the dataset. The syntax is the same. After reading the data from a csv file, we also want to drop certain column, or only select particular columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6647ce9d-3e5f-42ce-b836-21db648facaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df_X = read_df.drop(columns=\"label\").copy()\n",
    "read_df_y = read_df['label'].copy()\n",
    "print(read_df_X)\n",
    "print(read_df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73921d15-7cf7-487f-a809-48bca0524f98",
   "metadata": {},
   "source": [
    "We can stack different columns to create a bigger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89253b96-d52f-4b3b-a847-6f3484c80c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_pt1 = np.array([100, 120, 80, 70, 150, 200, 160, 250],'d')\n",
    "jet_eta1 = np.array([2.1, 1.5, -1.1, -2.5, 0.8, -0.4, -0.6, 0.9],'d')\n",
    "jet_eta1_and_pt1 = np.column_stack((jet_pt1, jet_eta1))\n",
    "df = pd.DataFrame(jet_eta1_and_pt1, columns=['jet_pt1', 'jet_eta1']) \n",
    "df.to_csv(\"dummy.csv\")\n",
    "read_df = pd.read_csv('dummy.csv')\n",
    "print(read_df)\n",
    "print(read_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db95684e-e15f-46ee-b38f-ad7b998c2eb5",
   "metadata": {},
   "source": [
    "### Awkward Array\n",
    "\n",
    "Everything is easy, right? The above examples can be handled with vanilla numpy sand pandas. One important feature of the example is that the dimension of the dataset is fixed. You may notice that numpy can only process this type of array. Let's say a failure example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780cdad7-804c-4c9b-ae3f-9850e1d24a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_pt1 = np.array([100, 120, 80, 70, 150, 200, 160, 250],'d')\n",
    "jet_eta1 = np.array([2.1, 1.5, -1.1, -2.5, 0.8, -0.4, -0.6],'d')\n",
    "jet_eta1_and_pt1 = np.column_stack((jet_pt1, jet_eta1))\n",
    "df = pd.DataFrame(jet_eta1_and_pt1, columns=['jet_pt1', 'jet_eta1']) \n",
    "df.to_csv(\"dummy.csv\")\n",
    "read_df = pd.read_csv('dummy.csv')\n",
    "print(read_df)\n",
    "print(read_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed74e1-58dc-4a27-97cd-8d9038d92bdd",
   "metadata": {},
   "source": [
    "This is a genuine problem as we have noticed that in HEP each event may contain various number of physics objects. Although we can pad the missing objects with NaN or other particular values, it is not convenient. A library that can handle variable length arrays is very useful in this field. Here comes the rescue, awkward array (https://awkward-array.org/doc/main/index.html). It is also called a jagged array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3659609f-7655-4133-928d-b23242a05e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "\n",
    "jet_eta1_and_pt1 = ak.Array([\n",
    "    [{\"jet_pt1\": 100, \"jet_eta1\": 2.1}], [{\"jet_pt1\": 120, \"jet_eta1\": 1.5}],\n",
    "    [{\"jet_pt1\": 80, \"jet_eta1\": -1.1}], [{\"jet_pt1\": 70, \"jet_eta1\": -2.5}],\n",
    "    [{\"jet_pt1\": 150, \"jet_eta1\": 0.8}], [{\"jet_pt1\": 200, \"jet_eta1\": -0.4}],\n",
    "    [{\"jet_pt1\": 160}], [{\"jet_pt1\": 250, \"jet_eta1\": -0.6}],\n",
    "])\n",
    "\n",
    "print(jet_eta1_and_pt1)\n",
    "print(jet_eta1_and_pt1[\"jet_pt1\"])\n",
    "print(jet_eta1_and_pt1[\"jet_eta1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596642ca-ea4a-4cec-9a57-56d167e79cd0",
   "metadata": {},
   "source": [
    "In the above example, we omitted the jet_eta1 for the second last entry, but the code did not throw and error, and it just added [None] to the missing element. Here we just use a very naive example. In reality, we should not have a dataset where one particular jet_eta1 is missing (it means the data integrity is compromised). But we can see from earlier tables that an event may contain different numbers of jets, etc. So if we have a library that can read common data format, converting it to jagged array efficiently, it will be very convenient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd3ad7-9c82-494b-a401-8185b736d7f2",
   "metadata": {},
   "source": [
    "### Uproot\n",
    "\n",
    "uproot (https://github.com/scikit-hep/uproot5) is a pure python-based library that allows us to work with ROOT files. In this tutorial, we attempt to demystify ROOT. Usually, the first data format concept you encounter in particle physics is ROOT (https://root.cern.ch/). A lot of the instructions introduce ROOT by explaining its basic structures such as branches, leaves, etc. But they are nothing special other than arrays. If you have understood the tables we added earlier, you have understood the basic structure of ROOT. Previously, one has to use C++ or pyROOT to deal with ROOT files, but with uproot, we can convert everything to jagged arrays, and embrace a pure python-based workflow.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbcef48-40f6-4843-88d5-c57ef7418e92",
   "metadata": {},
   "source": [
    "We have prepared a test Delphes output for you, which is a heavy particle (Z') decaying to two jets. The heavy particle mass is set to 1 TeV. So we are suppose to see a peak around 1 TeV when checking the invariant mass distribution of the jets. ROOT files store the data in TTrees. We will extract the jet info and save the 4-vectors in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59c670-c685-43fa-8b44-ccc29b9c2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following syntax to read a delphes root file and parse the branches to arrays\n",
    "# You might want to open the test file using ROOT to see the structure. \n",
    "# What is the tree name? How are all the branches defined?\n",
    "import uproot as r\n",
    "\n",
    "f = r.open(\"./data/delphes_zprime_1TeV.root\")\n",
    "delphes_tree = f[\"Delphes;1\"]\n",
    "\n",
    "pt = delphes_tree[\"Jet.PT\"].array()\n",
    "eta = delphes_tree[\"Jet.Eta\"].array()\n",
    "phi = delphes_tree[\"Jet.Phi\"].array()\n",
    "m = delphes_tree[\"Jet.Mass\"].array()               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7950ae04-a787-403a-a4b1-5df23eb0783d",
   "metadata": {},
   "source": [
    "All the numpy array syntax can be applied seamlessly to awkward array. But there are a few specific awkward functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00948b30-8404-4ff2-a0a3-3b8ee4197fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pt)\n",
    "print(pt.type)\n",
    "print(ak.num(pt))\n",
    "print(pt[ak.num(pt) > 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b81167-06af-4477-8619-00130ae50919",
   "metadata": {},
   "source": [
    "Ok so fine uproot can convert the ROOT files to arrays and everything can be done in python, but how about other precious ROOT libraries? TLorentzVectors, RooStats? Good question! The HEP community has made plenty of efforts to create python alternatives. In this tutorial, we will introduce the vector library (https://github.com/scikit-hep/vector), which support vector operations. It is a very important package to do data analysis in HEP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a9e69e-29a6-42f8-bc09-a50ac7831f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c40e6c-dbf0-4cd7-b696-5a9469e31c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.register_awkward()\n",
    "jet_vec = ak.zip({\n",
    "  \"pt\": pt,\n",
    "  \"phi\": phi,\n",
    "  \"eta\": eta,\n",
    "  \"mass\": m,\n",
    "},with_name=\"Momentum4D\")                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51231e58-1c37-4f32-b787-f9db182c90c4",
   "metadata": {},
   "source": [
    "Now let's check the constructed jet_vec object. We want to select events with at least two jets in order to get the invariant mass. The syntax is also very similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25e001-6fa7-4de4-9b05-de0fc165d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(jet_vec))\n",
    "jet_vec_select = jet_vec[ak.num(jet_vec) >= 2]\n",
    "print(len(jet_vec_select))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5940d0-c10d-4b26-815a-47849c55156d",
   "metadata": {},
   "source": [
    "Then let us get the leading and sub-leading jets. We can assign them to individual Lorentz vectors, after which all Lorentz vector operations supported by the vector library can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d87f32c-f6bd-4769-b6a8-c4e7e49c2782",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_jet = jet_vec_select[:,0] # https://stackoverflow.com/questions/16815928/what-does-mean-on-numpy-arrays\n",
    "sublead_jet = jet_vec_select[:,1]\n",
    "\n",
    "dijet = lead_jet + sublead_jet\n",
    "dijet_mass = dijet.mass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da04d09-912b-4d06-a139-6b70820ca1a2",
   "metadata": {},
   "source": [
    "## Visualising the data\n",
    "\n",
    "Visualising the data, or plotting, is a common task in HEP. You can hardly find any publications without a plot showing the data. ROOT has a quite extensive plotting suite, which can satisfy most of your need. But the rapid growing usage of ML in HEP encourages us to endorse a more broadly used package. matplotlib (https://matplotlib.org) is a python implementation of the matlab plotting functionalities, and it is the most popular plotting option among the new generation. \n",
    "\n",
    "You may think about what kind of plots are useful in HEP and how to make those in matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436df622-58c2-4ca7-83cc-ccdd361ac109",
   "metadata": {},
   "source": [
    "### Histogram\n",
    "\n",
    "You have to know how to make a histogram, without any doubts. A histogram represents the so-called \"binned\" data. Instead of showing the raw data, data points are grouped into bins. It reduces the statistical uncertainties and captures the big picture. Let's see a few examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ad57c-db7a-407b-a385-b9a0ea577344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "# The matplotlib hist function takes 1-D numpy arrays as the input \n",
    "# so a step converting awkward array to 1-D numpy is needed\n",
    "\n",
    "print(pt.type)\n",
    "print(ak.flatten(pt).type)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(ak.flatten(pt),bins=20,range=(0,1000)); #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "plt.xlabel(r'Jet $p_T$ (GeV/c)',fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pt[ak.num(pt) >= 1][:,0],bins=20,range=(0,1000));\n",
    "plt.xlabel(r'Leading Jet $p_T$ (GeV/c)',fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pt[ak.num(pt) >= 2][:,1],bins=20,range=(0,1000));\n",
    "plt.xlabel(r'Sub-leading Jet $p_T$ (GeV/c)',fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(dijet_mass,bins=20,range=(0,2000));\n",
    "plt.xlabel(r'Mass  (GeV/c)',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3293cc3a-9a68-4cab-95d1-53f716c13662",
   "metadata": {},
   "source": [
    "One important thing to remember is that the hist function returns the binning options, the event counts, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f9dc03-235c-4bd8-b596-e673903872e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins, patches = plt.hist(ak.flatten(pt),bins=20,range=(0,1000))\n",
    "print(counts)\n",
    "print(bins)\n",
    "print(patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a5048b-2c84-48f0-ae8d-b0379e6bac79",
   "metadata": {},
   "source": [
    "This means you can save the histogram info. You will realise in your career that usually a plot needs a few rounds of formatting before it can be published. If we can save the histogram data info, we just have to deal with the formatting later on. One good trick is to save a pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5d6b4-1057-4050-87cb-4417b9643b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "print(counts, bins)\n",
    "\n",
    "pickle.dump((counts, bins), open('pt.pkl', 'wb'))  \n",
    "\n",
    "counts_new, bins_new = pickle.load(open('pt.pkl', 'rb'))\n",
    "\n",
    "print(counts_new, bins_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a8b9a-bfc4-4a84-828d-73de1da18ee3",
   "metadata": {},
   "source": [
    "We have preserved the histogram data. If you need to completely remake the plot or combine this histogram with something else, it is a convenient way. Here we would like to emphasize the reproducibility in research. As we all know, it is the very essence of scientific research. It is greatly encouraged to build up such a habit to document what you have done and check point your intermediate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37261ccc-32d2-4d82-99c7-59d92bec7c47",
   "metadata": {},
   "source": [
    "### Scatter Plots\n",
    "\n",
    "When we have multiple variables, we often want to know if there is correlation between them. A scatter plot is the simplest method. As indicated, the data is scattered around a 2D plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2280968-0406-4174-953d-af4b424ae3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pt[ak.num(pt) >= 2][:,0], pt[ak.num(pt) >= 2][:,1])\n",
    "plt.xlabel(\"Leading jet pT [GeV]\")\n",
    "plt.ylabel(\"Sub-leading jet pT [GeV]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ff771-30fd-480c-8ab4-f1b6c03ee2f1",
   "metadata": {},
   "source": [
    "You can also make a 2D histogram. We leave this as your own exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83883f44-84b9-4eb8-a3bf-fc76e5f9629b",
   "metadata": {},
   "source": [
    "## Exercise -- Put everything together\n",
    "\n",
    "Alright, the above info should be sufficient for you to start exploring a specific topic. Here is the practice:\n",
    "\n",
    "In the data folder, you will find several signal samples and a background sample. Try to use what you have learned today to answer those questions:\n",
    "\n",
    "1. What is the main feature of the signal? How do those different signal processes compare?\n",
    "2. What variables can we construct to separate the signal from the background?\n",
    "3. What analysis strategy can you come up with? Could you use what we have introduced today to illustrate your reasoning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab49c99-a179-4bb9-a5b1-552298798469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
